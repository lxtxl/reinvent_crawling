이 실습 워크숍을 사용하면 PEFT (Parameter Efficiet Fine Tuning)를 사용하여 미세 조정 모델의 기술을 습득하여 개인화 사용 사례를 혁신 할 수 있습니다.S-Lora, Lorax 및 Punica와 같은 오픈 소스 기술로 뛰어 들어 고성능, 확장 가능한 LORA 어댑터의 교육 및 호스팅을 간소화하십시오.Amazon Sagemaker 최적화 기술의 힘을 발휘하고 추론 구성 요소 및 멀티 모델 엔드 포인트를 포함한 최적의 배포 전략을 발견하십시오.참여하려면 노트북을 가져와야합니다.
추론 최적화로 Amazon EK의 대형 언어 모델 (LLM)의 잠재력을 최대한 활용하십시오.
성능 및 비용 효율성.이 초크 토크는 배포 및 스케일링에 대한 실질적인 지침을 제공합니다.
AWS Fellentia, Karpenter 및 Kserve를 사용하여 EK의 Pytorch 기반 LLM.활용하는 방법을 배우십시오
추론을 가속화하고 대기 시간을 줄이며 비용을 낮추기위한 특수한 하드웨어.발견하다
Karpenter의 고급 자동 스케일링 기능이 리소스 활용을 최적화하고 변동을 처리하는 방법
워크로드.Kserve를 사용하여 효율적인 모델 배포 및 관리 기술을 마스터하십시오.실제 세계를 통해
예와 모범 사례, 고성능, 비용 효율적인 LLM 추론을 구축하기위한 전문 지식을 얻습니다.
파이프 라인.